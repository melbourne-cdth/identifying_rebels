{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who Are Our Rebels\n",
    "\n",
    "In this notebook I'm going to use some simple NLP to try to explore who were our favorite rebels. In the process I hope to demonstrate some of the data-wrangling challenges that go along with NLP.\n",
    "\n",
    "I have previously used the Canvas API to download the submissions for this assignment and used [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to extract the submission text. This was then saved as a JSON file, which is where this notebook begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import markdown\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rebel_text.json\", \"r\") as f:\n",
    "    rebel_text = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Format\n",
    "\n",
    "The data are loaded as a list of strings, each string being the text submitted by a student."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going to use the very popular [Spacy](https://spacy.io/) NLP.\n",
    "\n",
    "We will use Spacy's _named entity recognition_ functionality to identify proper names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entity Recognition\n",
    "\n",
    "Spacy will parse the sentences and then try to recognize different entitites that are named in the text, such as people or organizations or diseases. Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt in rebel_text:\n",
    "    doc = nlp(txt)\n",
    "    displacy.render(doc, style=\"ent\")\n",
    "    print('-'*72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy seems to do OK\n",
    "#### But there are some consistent failures\n",
    "\n",
    "For example\n",
    "\n",
    "- Nicolaus Copernicus identified as an organization (`ORG`)\n",
    "- Aristotle is identified as a product (`PRODUCT`, what product?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Entities\n",
    "\n",
    "The function `get_top_rebel` is what I use to try to identify the person each student is identifying as their rebel. Here are the assumptions I made.\n",
    "\n",
    "- A person might have been labeled as an organization, a person or a product.\n",
    "    - You can change the list of acceptable labels to see if you can get improved performance.\n",
    "- Because so many reference, Freeman Dyson (because of the assignment) I decided to filter him out of the responses.\n",
    "    - Sorry if he was your rebel\n",
    "- I assume all shorter names all refer to a longer name for which it is a substring\n",
    "    - For example, I assume all `\"Godfrey\"` references are referring to `\"Godfrey Hounsfield\"` the lon\n",
    "- I count which name is identifed the most often in a submission\n",
    "    - In case of a tie, I take the longer string as being the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_rebel(txt, labels=None):\n",
    "    if not labels:\n",
    "        labels = ['ORG', 'PERSON', 'PRODUCT']\n",
    "    doc = nlp(txt)\n",
    "    rtxts = [ent.string.strip() for ent in doc.ents if ent.label_ in labels and ent.string != 'Freeman' and ent.string != 'Dyson']\n",
    "    rtxts.sort(key=lambda f: len(f), reverse=True)\n",
    "    for i in range(len(rtxts)-1):\n",
    "        for j in range(i+1, len(rtxts)):\n",
    "            rr1 = rtxts[j]\n",
    "            rr0 = rtxts[i]\n",
    "            if rr1 in rr0:\n",
    "                rtxts[j] = rr0\n",
    "    c = Counter(rtxts)\n",
    "    top_count =  c.most_common(1)[0][1]\n",
    "    cc = [k for k,v in c.items() if v == top_count]\n",
    "    cc.sort(key=lambda f: len(f), reverse=True)\n",
    "    return cc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the rebel for each submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebels = []\n",
    "\n",
    "for txt in rebel_text:\n",
    "    top_rebel = get_top_rebel(txt)\n",
    "    rebels.append((top_rebel, txt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the submissions and matching rebels out as a Markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\"\"\"\n",
    "for r in rebels:\n",
    "    txt = txt + markdown.markdown(\"\"\"-------\\n## Text\\n %s\\n\\n### Identified Rebel: %s\\n\"\"\"%(r[1], r[0]))\n",
    "with open(\"results.md\", \"w\") as f:\n",
    "    f.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the identified Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted=Counter([r[0] for r in rebels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use [Seaborn](https://seaborn.pydata.org) to visualize our counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "data = pd.DataFrame(counted.items()).rename(columns={0:\"Rebel\", 1:\"Count\"}).sort_values(\"Rebel\", ascending=True)\n",
    "sns.set_color_codes(\"pastel\")\n",
    "\n",
    "sns.barplot(x=\"Count\", y=\"Rebel\", data=data, color=\"g\")\n",
    "#plt.yticks(rotation=45)\n",
    "f.savefig(\"identified_rebels.png\", dpi=300, facecolor='w', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "I took a fairly simplistic approach to identifying the named rebels. The technique was not robust to several textual features, such as typos and misspellings possessive form. Because I was counting mentions of names, if someone used a lot of pronouns to refer to the rebel I might not have identified them properly. Identify the answer you submitted. Did I correctly find your rebel? If not, can you think of things in your writing that could be edited to make the identification task easier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
